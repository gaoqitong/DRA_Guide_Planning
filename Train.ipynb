{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -372 | Episode: 0 | Qmax: 25.2632 | Exploration: 0.699300 | Step: 472 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 1 | Qmax: 31.7826 | Exploration: 0.698601 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 36 | Episode: 2 | Qmax: 31.8453 | Exploration: 0.697902 | Step: 64 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 3 | Qmax: 31.7882 | Exploration: 0.697204 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: -111 | Episode: 4 | Qmax: 35.1514 | Exploration: 0.696507 | Step: 211 | LearningRate: 0.00150 \n",
      "| Reward: -1099 | Episode: 5 | Qmax: 48.1120 | Exploration: 0.695810 | Step: 1199 | LearningRate: 0.00150 \n",
      "| Reward: 10 | Episode: 6 | Qmax: 56.0079 | Exploration: 0.695115 | Step: 90 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 7 | Qmax: 54.5748 | Exploration: 0.694420 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 8 | Qmax: 57.0191 | Exploration: 0.693725 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: -233 | Episode: 9 | Qmax: 53.6428 | Exploration: 0.693031 | Step: 333 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 10 | Qmax: 59.4780 | Exploration: 0.692338 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: -715 | Episode: 11 | Qmax: 58.6893 | Exploration: 0.691646 | Step: 815 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 12 | Qmax: 61.3352 | Exploration: 0.690954 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: 62 | Episode: 13 | Qmax: 56.5132 | Exploration: 0.690263 | Step: 38 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 14 | Qmax: 58.9242 | Exploration: 0.689573 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: -288 | Episode: 15 | Qmax: 61.2470 | Exploration: 0.688884 | Step: 388 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 16 | Qmax: 63.3862 | Exploration: 0.688195 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 0 | Episode: 17 | Qmax: 65.6267 | Exploration: 0.687507 | Step: 100 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 18 | Qmax: 66.8468 | Exploration: 0.686819 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: -69 | Episode: 19 | Qmax: 66.7118 | Exploration: 0.686132 | Step: 169 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 54 | Episode: 20 | Qmax: 67.8129 | Exploration: 0.685446 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 21 | Qmax: 68.0887 | Exploration: 0.684761 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 22 | Qmax: 66.8752 | Exploration: 0.684076 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 23 | Qmax: 68.2549 | Exploration: 0.683392 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 24 | Qmax: 72.2748 | Exploration: 0.682708 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: -88 | Episode: 25 | Qmax: 70.0150 | Exploration: 0.682026 | Step: 188 | LearningRate: 0.00150 \n",
      "| Reward: -2000 | Episode: 26 | Qmax: 71.1040 | Exploration: 0.682026 | Step: 1999 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 27 | Qmax: 73.4097 | Exploration: 0.681344 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 11 | Episode: 28 | Qmax: 72.9727 | Exploration: 0.680662 | Step: 89 | LearningRate: 0.00150 \n",
      "| Reward: -158 | Episode: 29 | Qmax: 75.3367 | Exploration: 0.679982 | Step: 258 | LearningRate: 0.00150 \n",
      "| Reward: -90 | Episode: 30 | Qmax: 74.3266 | Exploration: 0.679302 | Step: 190 | LearningRate: 0.00150 \n",
      "| Reward: 22 | Episode: 31 | Qmax: 79.5576 | Exploration: 0.678622 | Step: 78 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 32 | Qmax: 78.5176 | Exploration: 0.677944 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: -111 | Episode: 33 | Qmax: 78.1561 | Exploration: 0.677266 | Step: 211 | LearningRate: 0.00150 \n",
      "| Reward: -76 | Episode: 34 | Qmax: 80.7356 | Exploration: 0.676589 | Step: 176 | LearningRate: 0.00150 \n",
      "| Reward: -455 | Episode: 35 | Qmax: 77.6024 | Exploration: 0.675912 | Step: 555 | LearningRate: 0.00150 \n",
      "| Reward: -62 | Episode: 36 | Qmax: 78.0310 | Exploration: 0.675236 | Step: 162 | LearningRate: 0.00150 \n",
      "| Reward: -442 | Episode: 37 | Qmax: 76.3616 | Exploration: 0.674561 | Step: 542 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 38 | Qmax: 80.9529 | Exploration: 0.673886 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: -32 | Episode: 39 | Qmax: 80.7615 | Exploration: 0.673212 | Step: 132 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 34 | Episode: 40 | Qmax: 76.3830 | Exploration: 0.672539 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: -198 | Episode: 41 | Qmax: 82.6230 | Exploration: 0.671867 | Step: 298 | LearningRate: 0.00150 \n",
      "| Reward: -284 | Episode: 42 | Qmax: 80.5204 | Exploration: 0.671195 | Step: 384 | LearningRate: 0.00150 \n",
      "| Reward: 12 | Episode: 43 | Qmax: 84.1362 | Exploration: 0.670524 | Step: 88 | LearningRate: 0.00150 \n",
      "| Reward: -272 | Episode: 44 | Qmax: 81.8430 | Exploration: 0.669853 | Step: 372 | LearningRate: 0.00150 \n",
      "| Reward: -1522 | Episode: 45 | Qmax: 81.9221 | Exploration: 0.669183 | Step: 1622 | LearningRate: 0.00150 \n",
      "| Reward: -587 | Episode: 46 | Qmax: 83.3726 | Exploration: 0.668514 | Step: 687 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 47 | Qmax: 86.6800 | Exploration: 0.667845 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 27 | Episode: 48 | Qmax: 84.0786 | Exploration: 0.667178 | Step: 73 | LearningRate: 0.00150 \n",
      "| Reward: 1 | Episode: 49 | Qmax: 83.1351 | Exploration: 0.666510 | Step: 99 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 50 | Qmax: 88.2592 | Exploration: 0.665844 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: -95 | Episode: 51 | Qmax: 87.5673 | Exploration: 0.665178 | Step: 195 | LearningRate: 0.00150 \n",
      "| Reward: -71 | Episode: 52 | Qmax: 86.4974 | Exploration: 0.664513 | Step: 171 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 53 | Qmax: 87.1963 | Exploration: 0.663848 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: -500 | Episode: 54 | Qmax: 87.9146 | Exploration: 0.663185 | Step: 600 | LearningRate: 0.00150 \n",
      "| Reward: 25 | Episode: 55 | Qmax: 88.0848 | Exploration: 0.662521 | Step: 75 | LearningRate: 0.00150 \n",
      "| Reward: -124 | Episode: 56 | Qmax: 87.4133 | Exploration: 0.661859 | Step: 224 | LearningRate: 0.00150 \n",
      "| Reward: -384 | Episode: 57 | Qmax: 87.0037 | Exploration: 0.661197 | Step: 484 | LearningRate: 0.00150 \n",
      "| Reward: -6 | Episode: 58 | Qmax: 87.1554 | Exploration: 0.660536 | Step: 106 | LearningRate: 0.00150 \n",
      "| Reward: -144 | Episode: 59 | Qmax: 85.7433 | Exploration: 0.659875 | Step: 244 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -111 | Episode: 60 | Qmax: 87.0026 | Exploration: 0.659215 | Step: 211 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 61 | Qmax: 88.9671 | Exploration: 0.658556 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: -302 | Episode: 62 | Qmax: 90.3192 | Exploration: 0.657898 | Step: 402 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 63 | Qmax: 88.2810 | Exploration: 0.657240 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: -285 | Episode: 64 | Qmax: 90.2644 | Exploration: 0.656582 | Step: 385 | LearningRate: 0.00150 \n",
      "| Reward: -640 | Episode: 65 | Qmax: 88.9700 | Exploration: 0.655926 | Step: 740 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 66 | Qmax: 90.7341 | Exploration: 0.655270 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: -207 | Episode: 67 | Qmax: 89.4420 | Exploration: 0.654615 | Step: 307 | LearningRate: 0.00150 \n",
      "| Reward: 1 | Episode: 68 | Qmax: 91.9606 | Exploration: 0.653960 | Step: 99 | LearningRate: 0.00150 \n",
      "| Reward: -817 | Episode: 69 | Qmax: 89.1411 | Exploration: 0.653306 | Step: 917 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 70 | Qmax: 93.1556 | Exploration: 0.652653 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 38 | Episode: 71 | Qmax: 94.4508 | Exploration: 0.652000 | Step: 62 | LearningRate: 0.00150 \n",
      "| Reward: -1183 | Episode: 72 | Qmax: 90.8408 | Exploration: 0.651348 | Step: 1283 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 73 | Qmax: 94.1538 | Exploration: 0.650697 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 10 | Episode: 74 | Qmax: 91.6448 | Exploration: 0.650046 | Step: 90 | LearningRate: 0.00150 \n",
      "| Reward: -5 | Episode: 75 | Qmax: 89.9841 | Exploration: 0.649396 | Step: 105 | LearningRate: 0.00150 \n",
      "| Reward: -420 | Episode: 76 | Qmax: 90.6892 | Exploration: 0.648747 | Step: 520 | LearningRate: 0.00150 \n",
      "| Reward: -267 | Episode: 77 | Qmax: 89.9347 | Exploration: 0.648098 | Step: 367 | LearningRate: 0.00150 \n",
      "| Reward: -442 | Episode: 78 | Qmax: 90.5265 | Exploration: 0.647450 | Step: 542 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: -353 | Episode: 79 | Qmax: 91.1659 | Exploration: 0.646802 | Step: 453 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -418 | Episode: 80 | Qmax: 90.0664 | Exploration: 0.646156 | Step: 518 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 81 | Qmax: 92.5658 | Exploration: 0.645509 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: -374 | Episode: 82 | Qmax: 91.6211 | Exploration: 0.644864 | Step: 474 | LearningRate: 0.00150 \n",
      "| Reward: -1030 | Episode: 83 | Qmax: 90.4084 | Exploration: 0.644219 | Step: 1130 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 84 | Qmax: 91.5444 | Exploration: 0.643575 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: 62 | Episode: 85 | Qmax: 90.7383 | Exploration: 0.642931 | Step: 38 | LearningRate: 0.00150 \n",
      "| Reward: -142 | Episode: 86 | Qmax: 90.9318 | Exploration: 0.642288 | Step: 242 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 87 | Qmax: 93.7664 | Exploration: 0.641646 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 88 | Qmax: 94.0562 | Exploration: 0.641004 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 89 | Qmax: 93.6095 | Exploration: 0.640363 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: -69 | Episode: 90 | Qmax: 92.0119 | Exploration: 0.639723 | Step: 169 | LearningRate: 0.00150 \n",
      "| Reward: -108 | Episode: 91 | Qmax: 92.4266 | Exploration: 0.639083 | Step: 208 | LearningRate: 0.00150 \n",
      "| Reward: -83 | Episode: 92 | Qmax: 91.0969 | Exploration: 0.638444 | Step: 183 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 93 | Qmax: 92.3233 | Exploration: 0.637806 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 94 | Qmax: 93.5613 | Exploration: 0.637168 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 30 | Episode: 95 | Qmax: 90.6549 | Exploration: 0.636531 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: -245 | Episode: 96 | Qmax: 91.5568 | Exploration: 0.635894 | Step: 345 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 97 | Qmax: 91.6935 | Exploration: 0.635258 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 98 | Qmax: 93.2369 | Exploration: 0.634623 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 99 | Qmax: 93.4128 | Exploration: 0.633988 | Step: 63 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 11 | Episode: 100 | Qmax: 94.5422 | Exploration: 0.633355 | Step: 89 | LearningRate: 0.00150 \n",
      "| Reward: -218 | Episode: 101 | Qmax: 92.3461 | Exploration: 0.632721 | Step: 318 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 102 | Qmax: 93.0205 | Exploration: 0.632088 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 103 | Qmax: 93.4603 | Exploration: 0.631456 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: -4 | Episode: 104 | Qmax: 92.3659 | Exploration: 0.630825 | Step: 104 | LearningRate: 0.00150 \n",
      "| Reward: -138 | Episode: 105 | Qmax: 90.8327 | Exploration: 0.630194 | Step: 238 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 106 | Qmax: 92.0130 | Exploration: 0.629564 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 20 | Episode: 107 | Qmax: 92.5740 | Exploration: 0.628934 | Step: 80 | LearningRate: 0.00150 \n",
      "| Reward: 7 | Episode: 108 | Qmax: 93.9081 | Exploration: 0.628305 | Step: 93 | LearningRate: 0.00150 \n",
      "| Reward: -17 | Episode: 109 | Qmax: 94.9035 | Exploration: 0.627677 | Step: 117 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 110 | Qmax: 96.5800 | Exploration: 0.627049 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 111 | Qmax: 95.1617 | Exploration: 0.626422 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 19 | Episode: 112 | Qmax: 94.8761 | Exploration: 0.625796 | Step: 81 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 113 | Qmax: 95.7122 | Exploration: 0.625170 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 114 | Qmax: 94.8809 | Exploration: 0.624545 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 17 | Episode: 115 | Qmax: 93.5350 | Exploration: 0.623920 | Step: 83 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 116 | Qmax: 96.7164 | Exploration: 0.623296 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 38 | Episode: 117 | Qmax: 95.1020 | Exploration: 0.622673 | Step: 62 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 118 | Qmax: 95.5919 | Exploration: 0.622051 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: -17 | Episode: 119 | Qmax: 93.3255 | Exploration: 0.621428 | Step: 117 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 21 | Episode: 120 | Qmax: 95.0843 | Exploration: 0.620807 | Step: 79 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 121 | Qmax: 96.0527 | Exploration: 0.620186 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: -15 | Episode: 122 | Qmax: 92.9972 | Exploration: 0.619566 | Step: 115 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 123 | Qmax: 95.3838 | Exploration: 0.618946 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 8 | Episode: 124 | Qmax: 93.8013 | Exploration: 0.618328 | Step: 92 | LearningRate: 0.00150 \n",
      "| Reward: 2 | Episode: 125 | Qmax: 94.4963 | Exploration: 0.617709 | Step: 98 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 126 | Qmax: 95.6186 | Exploration: 0.617091 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: -314 | Episode: 127 | Qmax: 93.9872 | Exploration: 0.616474 | Step: 414 | LearningRate: 0.00150 \n",
      "| Reward: -386 | Episode: 128 | Qmax: 93.9286 | Exploration: 0.615858 | Step: 486 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 129 | Qmax: 95.1565 | Exploration: 0.615242 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 130 | Qmax: 96.5829 | Exploration: 0.614627 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 19 | Episode: 131 | Qmax: 94.9635 | Exploration: 0.614012 | Step: 81 | LearningRate: 0.00150 \n",
      "| Reward: 13 | Episode: 132 | Qmax: 94.6019 | Exploration: 0.613398 | Step: 87 | LearningRate: 0.00150 \n",
      "| Reward: 15 | Episode: 133 | Qmax: 95.3492 | Exploration: 0.612785 | Step: 85 | LearningRate: 0.00150 \n",
      "| Reward: -184 | Episode: 134 | Qmax: 93.8503 | Exploration: 0.612172 | Step: 284 | LearningRate: 0.00150 \n",
      "| Reward: 64 | Episode: 135 | Qmax: 96.8568 | Exploration: 0.611560 | Step: 36 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 136 | Qmax: 95.6441 | Exploration: 0.610948 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 137 | Qmax: 96.4775 | Exploration: 0.610337 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 138 | Qmax: 96.5373 | Exploration: 0.609727 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 58 | Episode: 139 | Qmax: 96.1268 | Exploration: 0.609117 | Step: 42 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 41 | Episode: 140 | Qmax: 93.8052 | Exploration: 0.608508 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 141 | Qmax: 96.3253 | Exploration: 0.607900 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 142 | Qmax: 96.6549 | Exploration: 0.607292 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 143 | Qmax: 95.5740 | Exploration: 0.606684 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 144 | Qmax: 95.2393 | Exploration: 0.606078 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 145 | Qmax: 96.0197 | Exploration: 0.605472 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 146 | Qmax: 95.4013 | Exploration: 0.604866 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 147 | Qmax: 93.3304 | Exploration: 0.604261 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: -34 | Episode: 148 | Qmax: 95.0537 | Exploration: 0.603657 | Step: 134 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 149 | Qmax: 96.2983 | Exploration: 0.603053 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 150 | Qmax: 96.5398 | Exploration: 0.602450 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 151 | Qmax: 94.7042 | Exploration: 0.601848 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 152 | Qmax: 96.3390 | Exploration: 0.601246 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 31 | Episode: 153 | Qmax: 95.2829 | Exploration: 0.600645 | Step: 69 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 154 | Qmax: 95.9882 | Exploration: 0.600044 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 155 | Qmax: 98.2475 | Exploration: 0.599444 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 156 | Qmax: 94.9093 | Exploration: 0.598845 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: -98 | Episode: 157 | Qmax: 94.5654 | Exploration: 0.598246 | Step: 198 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: -17 | Episode: 158 | Qmax: 95.0586 | Exploration: 0.597648 | Step: 117 | LearningRate: 0.00150 \n",
      "| Reward: -65 | Episode: 159 | Qmax: 94.6792 | Exploration: 0.597050 | Step: 165 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -123 | Episode: 160 | Qmax: 94.3301 | Exploration: 0.596453 | Step: 223 | LearningRate: 0.00150 \n",
      "| Reward: 66 | Episode: 161 | Qmax: 96.4763 | Exploration: 0.595856 | Step: 34 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 162 | Qmax: 95.3217 | Exploration: 0.595261 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 163 | Qmax: 96.8069 | Exploration: 0.594665 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 164 | Qmax: 97.6933 | Exploration: 0.594071 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 6 | Episode: 165 | Qmax: 94.9517 | Exploration: 0.593477 | Step: 94 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 166 | Qmax: 96.7201 | Exploration: 0.592883 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 31 | Episode: 167 | Qmax: 96.6316 | Exploration: 0.592290 | Step: 69 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 168 | Qmax: 95.9822 | Exploration: 0.591698 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 169 | Qmax: 96.3942 | Exploration: 0.591106 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 170 | Qmax: 95.3703 | Exploration: 0.590515 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: -46 | Episode: 171 | Qmax: 94.5053 | Exploration: 0.589925 | Step: 146 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 172 | Qmax: 94.9461 | Exploration: 0.589335 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 173 | Qmax: 96.7333 | Exploration: 0.588745 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 18 | Episode: 174 | Qmax: 96.4550 | Exploration: 0.588157 | Step: 82 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 175 | Qmax: 96.4503 | Exploration: 0.587568 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 176 | Qmax: 96.3094 | Exploration: 0.586981 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 177 | Qmax: 97.0366 | Exploration: 0.586394 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 14 | Episode: 178 | Qmax: 95.8010 | Exploration: 0.585808 | Step: 86 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 179 | Qmax: 96.1153 | Exploration: 0.585222 | Step: 56 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -3 | Episode: 180 | Qmax: 96.2284 | Exploration: 0.584636 | Step: 103 | LearningRate: 0.00150 \n",
      "| Reward: 61 | Episode: 181 | Qmax: 97.6567 | Exploration: 0.584052 | Step: 39 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 182 | Qmax: 96.4467 | Exploration: 0.583468 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 183 | Qmax: 97.2880 | Exploration: 0.582884 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 61 | Episode: 184 | Qmax: 97.5450 | Exploration: 0.582301 | Step: 39 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 185 | Qmax: 96.4076 | Exploration: 0.581719 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 186 | Qmax: 96.3236 | Exploration: 0.581137 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 15 | Episode: 187 | Qmax: 96.4120 | Exploration: 0.580556 | Step: 85 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 188 | Qmax: 96.8519 | Exploration: 0.579976 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 189 | Qmax: 95.7702 | Exploration: 0.579396 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 190 | Qmax: 96.2933 | Exploration: 0.578816 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 191 | Qmax: 97.1145 | Exploration: 0.578238 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 192 | Qmax: 95.8989 | Exploration: 0.577659 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 193 | Qmax: 96.6181 | Exploration: 0.577082 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 194 | Qmax: 96.3697 | Exploration: 0.576505 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: -1 | Episode: 195 | Qmax: 95.3258 | Exploration: 0.575928 | Step: 101 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 196 | Qmax: 95.5800 | Exploration: 0.575352 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 28 | Episode: 197 | Qmax: 96.4859 | Exploration: 0.574777 | Step: 72 | LearningRate: 0.00150 \n",
      "| Reward: 2 | Episode: 198 | Qmax: 97.0376 | Exploration: 0.574202 | Step: 98 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 199 | Qmax: 97.5329 | Exploration: 0.573628 | Step: 44 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 10 | Episode: 200 | Qmax: 95.6087 | Exploration: 0.573054 | Step: 90 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 201 | Qmax: 97.8101 | Exploration: 0.572481 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 202 | Qmax: 97.5283 | Exploration: 0.571909 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 203 | Qmax: 97.1972 | Exploration: 0.571337 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 30 | Episode: 204 | Qmax: 95.6918 | Exploration: 0.570765 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 205 | Qmax: 96.3388 | Exploration: 0.570195 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 206 | Qmax: 96.3236 | Exploration: 0.569624 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 207 | Qmax: 98.0959 | Exploration: 0.569055 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 31 | Episode: 208 | Qmax: 96.8145 | Exploration: 0.568486 | Step: 69 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 209 | Qmax: 96.2709 | Exploration: 0.567917 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: -59 | Episode: 210 | Qmax: 96.5485 | Exploration: 0.567349 | Step: 159 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 211 | Qmax: 97.2238 | Exploration: 0.566782 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: 41 | Episode: 212 | Qmax: 96.5089 | Exploration: 0.566215 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 213 | Qmax: 96.7543 | Exploration: 0.565649 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 214 | Qmax: 96.3899 | Exploration: 0.565083 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 215 | Qmax: 98.1044 | Exploration: 0.564518 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 20 | Episode: 216 | Qmax: 97.2578 | Exploration: 0.563954 | Step: 80 | LearningRate: 0.00150 \n",
      "| Reward: -47 | Episode: 217 | Qmax: 95.0167 | Exploration: 0.563390 | Step: 147 | LearningRate: 0.00150 \n",
      "| Reward: 58 | Episode: 218 | Qmax: 98.8330 | Exploration: 0.562826 | Step: 42 | LearningRate: 0.00150 \n",
      "| Reward: -2 | Episode: 219 | Qmax: 95.4926 | Exploration: 0.562264 | Step: 102 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 39 | Episode: 220 | Qmax: 96.2140 | Exploration: 0.561701 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 221 | Qmax: 98.1680 | Exploration: 0.561140 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: -4 | Episode: 222 | Qmax: 96.5827 | Exploration: 0.560578 | Step: 104 | LearningRate: 0.00150 \n",
      "| Reward: 38 | Episode: 223 | Qmax: 96.6619 | Exploration: 0.560018 | Step: 62 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 224 | Qmax: 97.1800 | Exploration: 0.559458 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: -125 | Episode: 225 | Qmax: 96.0871 | Exploration: 0.558898 | Step: 225 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 226 | Qmax: 96.6298 | Exploration: 0.558340 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 227 | Qmax: 97.0589 | Exploration: 0.557781 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 228 | Qmax: 96.3315 | Exploration: 0.557223 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 229 | Qmax: 98.3341 | Exploration: 0.556666 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 230 | Qmax: 98.5434 | Exploration: 0.556110 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 231 | Qmax: 97.3088 | Exploration: 0.555553 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 232 | Qmax: 97.5691 | Exploration: 0.554998 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 233 | Qmax: 96.6251 | Exploration: 0.554443 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 234 | Qmax: 96.7896 | Exploration: 0.553888 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 235 | Qmax: 97.4953 | Exploration: 0.553335 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: -105 | Episode: 236 | Qmax: 96.2610 | Exploration: 0.552781 | Step: 205 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: -2 | Episode: 237 | Qmax: 96.0386 | Exploration: 0.552228 | Step: 102 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 238 | Qmax: 97.5772 | Exploration: 0.551676 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: -3 | Episode: 239 | Qmax: 96.9371 | Exploration: 0.551125 | Step: 103 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -20 | Episode: 240 | Qmax: 95.9793 | Exploration: 0.550573 | Step: 120 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 241 | Qmax: 96.3321 | Exploration: 0.550023 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: -17 | Episode: 242 | Qmax: 96.7077 | Exploration: 0.549473 | Step: 117 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 243 | Qmax: 99.2273 | Exploration: 0.548923 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 244 | Qmax: 96.7244 | Exploration: 0.548374 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: 59 | Episode: 245 | Qmax: 97.9531 | Exploration: 0.547826 | Step: 41 | LearningRate: 0.00150 \n",
      "| Reward: 63 | Episode: 246 | Qmax: 98.9821 | Exploration: 0.547278 | Step: 37 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 247 | Qmax: 96.2364 | Exploration: 0.546731 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 5 | Episode: 248 | Qmax: 96.3903 | Exploration: 0.546184 | Step: 95 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 249 | Qmax: 97.0964 | Exploration: 0.545638 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 250 | Qmax: 98.2534 | Exploration: 0.545092 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 251 | Qmax: 98.4888 | Exploration: 0.544547 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 27 | Episode: 252 | Qmax: 96.2276 | Exploration: 0.544003 | Step: 73 | LearningRate: 0.00150 \n",
      "| Reward: -4 | Episode: 253 | Qmax: 96.1789 | Exploration: 0.543459 | Step: 104 | LearningRate: 0.00150 \n",
      "| Reward: 28 | Episode: 254 | Qmax: 96.0220 | Exploration: 0.542915 | Step: 72 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 255 | Qmax: 98.2496 | Exploration: 0.542372 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 21 | Episode: 256 | Qmax: 96.9724 | Exploration: 0.541830 | Step: 79 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 257 | Qmax: 96.3482 | Exploration: 0.541288 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 30 | Episode: 258 | Qmax: 97.3927 | Exploration: 0.540747 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: -58 | Episode: 259 | Qmax: 96.5739 | Exploration: 0.540206 | Step: 158 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 37 | Episode: 260 | Qmax: 97.7871 | Exploration: 0.539666 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 261 | Qmax: 96.9165 | Exploration: 0.539126 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 28 | Episode: 262 | Qmax: 96.6202 | Exploration: 0.538587 | Step: 72 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 263 | Qmax: 97.7637 | Exploration: 0.538049 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 264 | Qmax: 98.0428 | Exploration: 0.537510 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 265 | Qmax: 96.6029 | Exploration: 0.536973 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 266 | Qmax: 97.2127 | Exploration: 0.536436 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 41 | Episode: 267 | Qmax: 97.5929 | Exploration: 0.535900 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 268 | Qmax: 96.2324 | Exploration: 0.535364 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 269 | Qmax: 96.7390 | Exploration: 0.534828 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 270 | Qmax: 97.2809 | Exploration: 0.534293 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 271 | Qmax: 97.0228 | Exploration: 0.533759 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 272 | Qmax: 96.8635 | Exploration: 0.533225 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 273 | Qmax: 96.7181 | Exploration: 0.532692 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 274 | Qmax: 97.0683 | Exploration: 0.532159 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 275 | Qmax: 98.1006 | Exploration: 0.531627 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 276 | Qmax: 96.0563 | Exploration: 0.531096 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: -26 | Episode: 277 | Qmax: 96.6018 | Exploration: 0.530565 | Step: 126 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 278 | Qmax: 96.5174 | Exploration: 0.530034 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: -69 | Episode: 279 | Qmax: 95.8586 | Exploration: 0.529504 | Step: 169 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 0 | Episode: 280 | Qmax: 96.1201 | Exploration: 0.528975 | Step: 100 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 281 | Qmax: 98.1704 | Exploration: 0.528446 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 282 | Qmax: 96.8396 | Exploration: 0.527917 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 283 | Qmax: 97.6382 | Exploration: 0.527389 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 284 | Qmax: 97.2623 | Exploration: 0.526862 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 64 | Episode: 285 | Qmax: 99.4090 | Exploration: 0.526335 | Step: 36 | LearningRate: 0.00150 \n",
      "| Reward: -45 | Episode: 286 | Qmax: 95.6393 | Exploration: 0.525809 | Step: 145 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 287 | Qmax: 97.2804 | Exploration: 0.525283 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 288 | Qmax: 98.0516 | Exploration: 0.524757 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 289 | Qmax: 97.6457 | Exploration: 0.524233 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 290 | Qmax: 97.3089 | Exploration: 0.523709 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 4 | Episode: 291 | Qmax: 97.2294 | Exploration: 0.523185 | Step: 96 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 292 | Qmax: 97.4557 | Exploration: 0.522662 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 293 | Qmax: 97.5220 | Exploration: 0.522139 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 294 | Qmax: 98.6244 | Exploration: 0.521617 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 295 | Qmax: 96.8952 | Exploration: 0.521095 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: -1 | Episode: 296 | Qmax: 96.8253 | Exploration: 0.520574 | Step: 101 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 297 | Qmax: 97.8595 | Exploration: 0.520054 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 298 | Qmax: 97.5909 | Exploration: 0.519533 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 299 | Qmax: 95.8722 | Exploration: 0.519014 | Step: 67 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 31 | Episode: 300 | Qmax: 96.6468 | Exploration: 0.518495 | Step: 69 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 301 | Qmax: 97.9878 | Exploration: 0.517976 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 302 | Qmax: 97.1615 | Exploration: 0.517458 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 303 | Qmax: 98.0445 | Exploration: 0.516941 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 304 | Qmax: 97.1163 | Exploration: 0.516424 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 11 | Episode: 305 | Qmax: 97.1801 | Exploration: 0.515908 | Step: 89 | LearningRate: 0.00150 \n",
      "| Reward: -89 | Episode: 306 | Qmax: 95.6415 | Exploration: 0.515392 | Step: 189 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 307 | Qmax: 97.6695 | Exploration: 0.514876 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: -26 | Episode: 308 | Qmax: 96.7061 | Exploration: 0.514361 | Step: 126 | LearningRate: 0.00150 \n",
      "| Reward: 22 | Episode: 309 | Qmax: 96.7744 | Exploration: 0.513847 | Step: 78 | LearningRate: 0.00150 \n",
      "| Reward: 31 | Episode: 310 | Qmax: 95.3836 | Exploration: 0.513333 | Step: 69 | LearningRate: 0.00150 \n",
      "| Reward: 64 | Episode: 311 | Qmax: 97.6170 | Exploration: 0.512820 | Step: 36 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 312 | Qmax: 97.1269 | Exploration: 0.512307 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 12 | Episode: 313 | Qmax: 96.2493 | Exploration: 0.511795 | Step: 88 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 314 | Qmax: 97.0907 | Exploration: 0.511283 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 315 | Qmax: 97.4778 | Exploration: 0.510772 | Step: 53 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: -12 | Episode: 316 | Qmax: 96.9636 | Exploration: 0.510261 | Step: 112 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 317 | Qmax: 96.4730 | Exploration: 0.509751 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 318 | Qmax: 97.4877 | Exploration: 0.509241 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 61 | Episode: 319 | Qmax: 98.1423 | Exploration: 0.508732 | Step: 39 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 43 | Episode: 320 | Qmax: 97.2742 | Exploration: 0.508223 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 321 | Qmax: 97.8544 | Exploration: 0.507715 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 322 | Qmax: 96.4940 | Exploration: 0.507207 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 3 | Episode: 323 | Qmax: 96.9700 | Exploration: 0.506700 | Step: 97 | LearningRate: 0.00150 \n",
      "| Reward: 36 | Episode: 324 | Qmax: 97.0515 | Exploration: 0.506193 | Step: 64 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 325 | Qmax: 97.3520 | Exploration: 0.505687 | Step: 67 | LearningRate: 0.00150 \n",
      "| Reward: 28 | Episode: 326 | Qmax: 96.4840 | Exploration: 0.505181 | Step: 72 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 327 | Qmax: 97.2791 | Exploration: 0.504676 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 328 | Qmax: 96.7833 | Exploration: 0.504171 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: 65 | Episode: 329 | Qmax: 98.1579 | Exploration: 0.503667 | Step: 35 | LearningRate: 0.00150 \n",
      "| Reward: -2 | Episode: 330 | Qmax: 96.6780 | Exploration: 0.503164 | Step: 102 | LearningRate: 0.00150 \n",
      "| Reward: 56 | Episode: 331 | Qmax: 97.1997 | Exploration: 0.502660 | Step: 44 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 332 | Qmax: 98.0232 | Exploration: 0.502158 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 36 | Episode: 333 | Qmax: 97.5527 | Exploration: 0.501656 | Step: 64 | LearningRate: 0.00150 \n",
      "| Reward: 24 | Episode: 334 | Qmax: 96.3864 | Exploration: 0.501154 | Step: 76 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 335 | Qmax: 98.0899 | Exploration: 0.500653 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 336 | Qmax: 97.0722 | Exploration: 0.500152 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 337 | Qmax: 97.9289 | Exploration: 0.499652 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 36 | Episode: 338 | Qmax: 97.6218 | Exploration: 0.499152 | Step: 64 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 339 | Qmax: 98.2999 | Exploration: 0.498653 | Step: 65 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 25 | Episode: 340 | Qmax: 96.4961 | Exploration: 0.498154 | Step: 75 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 341 | Qmax: 97.5934 | Exploration: 0.497656 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 342 | Qmax: 96.3474 | Exploration: 0.497159 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: -31 | Episode: 343 | Qmax: 96.5316 | Exploration: 0.496662 | Step: 131 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 344 | Qmax: 97.2820 | Exploration: 0.496165 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 21 | Episode: 345 | Qmax: 97.2749 | Exploration: 0.495669 | Step: 79 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 346 | Qmax: 97.1301 | Exploration: 0.495173 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 347 | Qmax: 96.7413 | Exploration: 0.494678 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 348 | Qmax: 98.0799 | Exploration: 0.494183 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 349 | Qmax: 97.9891 | Exploration: 0.493689 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 350 | Qmax: 98.0968 | Exploration: 0.493195 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 351 | Qmax: 97.2248 | Exploration: 0.492702 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 352 | Qmax: 97.7380 | Exploration: 0.492209 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 353 | Qmax: 97.1877 | Exploration: 0.491717 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 354 | Qmax: 98.0361 | Exploration: 0.491225 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 30 | Episode: 355 | Qmax: 97.0089 | Exploration: 0.490734 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 356 | Qmax: 97.8213 | Exploration: 0.490244 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: -110 | Episode: 357 | Qmax: 96.3584 | Exploration: 0.489753 | Step: 210 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 358 | Qmax: 99.3964 | Exploration: 0.489264 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 359 | Qmax: 96.7737 | Exploration: 0.488774 | Step: 56 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 49 | Episode: 360 | Qmax: 97.6194 | Exploration: 0.488285 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 361 | Qmax: 97.8366 | Exploration: 0.487797 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 362 | Qmax: 98.0429 | Exploration: 0.487309 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 17 | Episode: 363 | Qmax: 96.4798 | Exploration: 0.486822 | Step: 83 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 364 | Qmax: 98.0188 | Exploration: 0.486335 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 30 | Episode: 365 | Qmax: 97.9581 | Exploration: 0.485849 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: 15 | Episode: 366 | Qmax: 97.7214 | Exploration: 0.485363 | Step: 85 | LearningRate: 0.00150 \n",
      "| Reward: 20 | Episode: 367 | Qmax: 97.4232 | Exploration: 0.484878 | Step: 80 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 368 | Qmax: 97.6994 | Exploration: 0.484393 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 369 | Qmax: 97.8143 | Exploration: 0.483908 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 370 | Qmax: 99.1666 | Exploration: 0.483425 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 371 | Qmax: 97.4251 | Exploration: 0.482941 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: 58 | Episode: 372 | Qmax: 98.1310 | Exploration: 0.482458 | Step: 42 | LearningRate: 0.00150 \n",
      "| Reward: -19 | Episode: 373 | Qmax: 96.9883 | Exploration: 0.481976 | Step: 119 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 374 | Qmax: 98.0189 | Exploration: 0.481494 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 58 | Episode: 375 | Qmax: 98.8311 | Exploration: 0.481012 | Step: 42 | LearningRate: 0.00150 \n",
      "| Reward: 24 | Episode: 376 | Qmax: 97.3752 | Exploration: 0.480531 | Step: 76 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 377 | Qmax: 98.2023 | Exploration: 0.480051 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 378 | Qmax: 97.4738 | Exploration: 0.479571 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 379 | Qmax: 98.5558 | Exploration: 0.479091 | Step: 45 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: -2 | Episode: 380 | Qmax: 96.8193 | Exploration: 0.478612 | Step: 102 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 381 | Qmax: 97.1016 | Exploration: 0.478133 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 382 | Qmax: 97.3768 | Exploration: 0.477655 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 383 | Qmax: 98.7755 | Exploration: 0.477178 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 384 | Qmax: 97.5768 | Exploration: 0.476700 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 27 | Episode: 385 | Qmax: 97.3652 | Exploration: 0.476224 | Step: 73 | LearningRate: 0.00150 \n",
      "| Reward: 38 | Episode: 386 | Qmax: 97.8063 | Exploration: 0.475747 | Step: 62 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 387 | Qmax: 98.2885 | Exploration: 0.475272 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 67 | Episode: 388 | Qmax: 99.0209 | Exploration: 0.474796 | Step: 33 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 389 | Qmax: 97.9973 | Exploration: 0.474322 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 390 | Qmax: 98.7883 | Exploration: 0.473847 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 391 | Qmax: 98.5452 | Exploration: 0.473373 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 57 | Episode: 392 | Qmax: 98.5209 | Exploration: 0.472900 | Step: 43 | LearningRate: 0.00150 \n",
      "| Reward: 4 | Episode: 393 | Qmax: 97.2514 | Exploration: 0.472427 | Step: 96 | LearningRate: 0.00150 \n",
      "| Reward: 22 | Episode: 394 | Qmax: 97.8437 | Exploration: 0.471955 | Step: 78 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: 45 | Episode: 395 | Qmax: 96.6010 | Exploration: 0.471483 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 396 | Qmax: 97.1089 | Exploration: 0.471011 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 397 | Qmax: 99.0614 | Exploration: 0.470540 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 19 | Episode: 398 | Qmax: 96.8103 | Exploration: 0.470070 | Step: 81 | LearningRate: 0.00150 \n",
      "| Reward: 24 | Episode: 399 | Qmax: 97.7708 | Exploration: 0.469600 | Step: 76 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 59 | Episode: 400 | Qmax: 98.4619 | Exploration: 0.469130 | Step: 41 | LearningRate: 0.00150 \n",
      "| Reward: 38 | Episode: 401 | Qmax: 96.5812 | Exploration: 0.468661 | Step: 62 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 402 | Qmax: 97.5338 | Exploration: 0.468192 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 403 | Qmax: 97.7938 | Exploration: 0.467724 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 32 | Episode: 404 | Qmax: 97.9739 | Exploration: 0.467256 | Step: 68 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 405 | Qmax: 98.4291 | Exploration: 0.466789 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 406 | Qmax: 98.7048 | Exploration: 0.466322 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 27 | Episode: 407 | Qmax: 97.7144 | Exploration: 0.465856 | Step: 73 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 408 | Qmax: 98.3319 | Exploration: 0.465390 | Step: 66 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 409 | Qmax: 98.1469 | Exploration: 0.464925 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 410 | Qmax: 98.0428 | Exploration: 0.464460 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: 53 | Episode: 411 | Qmax: 98.2778 | Exploration: 0.463995 | Step: 47 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 412 | Qmax: 97.6620 | Exploration: 0.463531 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 413 | Qmax: 97.1223 | Exploration: 0.463068 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 414 | Qmax: 98.2551 | Exploration: 0.462605 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 415 | Qmax: 98.2999 | Exploration: 0.462142 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 416 | Qmax: 98.6657 | Exploration: 0.461680 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 417 | Qmax: 97.2494 | Exploration: 0.461218 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 21 | Episode: 418 | Qmax: 98.6398 | Exploration: 0.460757 | Step: 79 | LearningRate: 0.00150 \n",
      "| Reward: 2 | Episode: 419 | Qmax: 96.5765 | Exploration: 0.460296 | Step: 98 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 61 | Episode: 420 | Qmax: 99.4041 | Exploration: 0.459836 | Step: 39 | LearningRate: 0.00150 \n",
      "| Reward: 20 | Episode: 421 | Qmax: 97.2548 | Exploration: 0.459376 | Step: 80 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 422 | Qmax: 98.1573 | Exploration: 0.458917 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 423 | Qmax: 98.5092 | Exploration: 0.458458 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: -15 | Episode: 424 | Qmax: 97.1236 | Exploration: 0.458000 | Step: 115 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 425 | Qmax: 98.1101 | Exploration: 0.457542 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 426 | Qmax: 98.4017 | Exploration: 0.457084 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 427 | Qmax: 98.1886 | Exploration: 0.456627 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: 42 | Episode: 428 | Qmax: 98.0703 | Exploration: 0.456170 | Step: 58 | LearningRate: 0.00150 \n",
      "| Reward: 61 | Episode: 429 | Qmax: 98.4632 | Exploration: 0.455714 | Step: 39 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 430 | Qmax: 98.6429 | Exploration: 0.455258 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 431 | Qmax: 98.4373 | Exploration: 0.454803 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 432 | Qmax: 98.4672 | Exploration: 0.454348 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 433 | Qmax: 98.0357 | Exploration: 0.453894 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: -44 | Episode: 434 | Qmax: 97.3252 | Exploration: 0.453440 | Step: 144 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 435 | Qmax: 97.6655 | Exploration: 0.452987 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 41 | Episode: 436 | Qmax: 98.4725 | Exploration: 0.452534 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 437 | Qmax: 97.4659 | Exploration: 0.452081 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 2 | Episode: 438 | Qmax: 97.2760 | Exploration: 0.451629 | Step: 98 | LearningRate: 0.00150 \n",
      "| Reward: -55 | Episode: 439 | Qmax: 97.0862 | Exploration: 0.451177 | Step: 155 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 51 | Episode: 440 | Qmax: 98.6725 | Exploration: 0.450726 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 441 | Qmax: 98.3342 | Exploration: 0.450276 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 442 | Qmax: 98.1364 | Exploration: 0.449825 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 24 | Episode: 443 | Qmax: 97.7168 | Exploration: 0.449375 | Step: 76 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 444 | Qmax: 98.3361 | Exploration: 0.448926 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 8 | Episode: 445 | Qmax: 96.9185 | Exploration: 0.448477 | Step: 92 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 446 | Qmax: 97.2678 | Exploration: 0.448029 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 24 | Episode: 447 | Qmax: 96.9305 | Exploration: 0.447581 | Step: 76 | LearningRate: 0.00150 \n",
      "| Reward: 35 | Episode: 448 | Qmax: 97.6637 | Exploration: 0.447133 | Step: 65 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 449 | Qmax: 98.2812 | Exploration: 0.446686 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: 0 | Episode: 450 | Qmax: 96.5401 | Exploration: 0.446239 | Step: 100 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 451 | Qmax: 98.4884 | Exploration: 0.445793 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 48 | Episode: 452 | Qmax: 97.6757 | Exploration: 0.445347 | Step: 52 | LearningRate: 0.00150 \n",
      "| Reward: 43 | Episode: 453 | Qmax: 97.5008 | Exploration: 0.444902 | Step: 57 | LearningRate: 0.00150 \n",
      "| Reward: 55 | Episode: 454 | Qmax: 97.4984 | Exploration: 0.444457 | Step: 45 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 455 | Qmax: 97.2856 | Exploration: 0.444012 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 456 | Qmax: 97.4428 | Exploration: 0.443568 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 457 | Qmax: 98.2563 | Exploration: 0.443125 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 58 | Episode: 458 | Qmax: 98.0677 | Exploration: 0.442682 | Step: 42 | LearningRate: 0.00150 \n",
      "| Reward: 28 | Episode: 459 | Qmax: 98.0061 | Exploration: 0.442239 | Step: 72 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 60 | Episode: 460 | Qmax: 98.7958 | Exploration: 0.441797 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 61 | Episode: 461 | Qmax: 98.7405 | Exploration: 0.441355 | Step: 39 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 462 | Qmax: 98.0050 | Exploration: 0.440914 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 19 | Episode: 463 | Qmax: 97.7369 | Exploration: 0.440473 | Step: 81 | LearningRate: 0.00150 \n",
      "| Reward: 63 | Episode: 464 | Qmax: 99.1531 | Exploration: 0.440032 | Step: 37 | LearningRate: 0.00150 \n",
      "| Reward: -7 | Episode: 465 | Qmax: 97.7831 | Exploration: 0.439592 | Step: 107 | LearningRate: 0.00150 \n",
      "| Reward: 50 | Episode: 466 | Qmax: 98.6575 | Exploration: 0.439153 | Step: 50 | LearningRate: 0.00150 \n",
      "| Reward: -5 | Episode: 467 | Qmax: 98.4645 | Exploration: 0.438714 | Step: 105 | LearningRate: 0.00150 \n",
      "| Reward: 23 | Episode: 468 | Qmax: 99.1032 | Exploration: 0.438275 | Step: 77 | LearningRate: 0.00150 \n",
      "| Reward: -9 | Episode: 469 | Qmax: 98.2410 | Exploration: 0.437837 | Step: 109 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 470 | Qmax: 97.9605 | Exploration: 0.437399 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: 52 | Episode: 471 | Qmax: 98.0013 | Exploration: 0.436961 | Step: 48 | LearningRate: 0.00150 \n",
      "| Reward: 60 | Episode: 472 | Qmax: 100.9614 | Exploration: 0.436524 | Step: 40 | LearningRate: 0.00150 \n",
      "| Reward: 33 | Episode: 473 | Qmax: 98.8542 | Exploration: 0.436088 | Step: 67 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: 13 | Episode: 474 | Qmax: 97.2762 | Exploration: 0.435652 | Step: 87 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 475 | Qmax: 98.3133 | Exploration: 0.435216 | Step: 56 | LearningRate: 0.00150 \n",
      "| Reward: -14 | Episode: 476 | Qmax: 98.1874 | Exploration: 0.434781 | Step: 114 | LearningRate: 0.00150 \n",
      "| Reward: -26 | Episode: 477 | Qmax: 99.2676 | Exploration: 0.434346 | Step: 126 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 478 | Qmax: 101.4826 | Exploration: 0.433912 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: 34 | Episode: 479 | Qmax: 98.8858 | Exploration: 0.433478 | Step: 66 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 30 | Episode: 480 | Qmax: 100.2446 | Exploration: 0.433044 | Step: 70 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 481 | Qmax: 99.0019 | Exploration: 0.432611 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: -2 | Episode: 482 | Qmax: 97.9620 | Exploration: 0.432179 | Step: 102 | LearningRate: 0.00150 \n",
      "| Reward: 37 | Episode: 483 | Qmax: 98.5731 | Exploration: 0.431747 | Step: 63 | LearningRate: 0.00150 \n",
      "| Reward: 11 | Episode: 484 | Qmax: 97.9097 | Exploration: 0.431315 | Step: 89 | LearningRate: 0.00150 \n",
      "| Reward: -89 | Episode: 485 | Qmax: 98.3074 | Exploration: 0.430883 | Step: 189 | LearningRate: 0.00150 \n",
      "| Reward: 13 | Episode: 486 | Qmax: 100.0691 | Exploration: 0.430453 | Step: 87 | LearningRate: 0.00150 \n",
      "| Reward: 9 | Episode: 487 | Qmax: 98.2076 | Exploration: 0.430022 | Step: 91 | LearningRate: 0.00150 \n",
      "| Reward: 54 | Episode: 488 | Qmax: 99.7946 | Exploration: 0.429592 | Step: 46 | LearningRate: 0.00150 \n",
      "| Reward: 49 | Episode: 489 | Qmax: 99.3165 | Exploration: 0.429163 | Step: 51 | LearningRate: 0.00150 \n",
      "| Reward: 40 | Episode: 490 | Qmax: 99.7023 | Exploration: 0.428733 | Step: 60 | LearningRate: 0.00150 \n",
      "| Reward: 45 | Episode: 491 | Qmax: 98.6837 | Exploration: 0.428305 | Step: 55 | LearningRate: 0.00150 \n",
      "| Reward: 68 | Episode: 492 | Qmax: 101.4997 | Exploration: 0.427876 | Step: 32 | LearningRate: 0.00150 \n",
      "| Reward: 41 | Episode: 493 | Qmax: 99.5271 | Exploration: 0.427448 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: -43 | Episode: 494 | Qmax: 98.1843 | Exploration: 0.427021 | Step: 143 | LearningRate: 0.00150 \n",
      "| Reward: 8 | Episode: 495 | Qmax: 97.7219 | Exploration: 0.426594 | Step: 92 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 496 | Qmax: 99.2041 | Exploration: 0.426167 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: 51 | Episode: 497 | Qmax: 100.9508 | Exploration: 0.425741 | Step: 49 | LearningRate: 0.00150 \n",
      "| Reward: -11 | Episode: 498 | Qmax: 99.6092 | Exploration: 0.425315 | Step: 111 | LearningRate: 0.00150 \n",
      "| Reward: 44 | Episode: 499 | Qmax: 97.9123 | Exploration: 0.424890 | Step: 56 | LearningRate: 0.00150 \n",
      "DDQN Saved\n",
      "| Reward: 41 | Episode: 500 | Qmax: 100.0615 | Exploration: 0.424465 | Step: 59 | LearningRate: 0.00150 \n",
      "| Reward: 26 | Episode: 501 | Qmax: 99.0277 | Exploration: 0.424041 | Step: 74 | LearningRate: 0.00150 \n",
      "| Reward: -6 | Episode: 502 | Qmax: 99.6101 | Exploration: 0.423617 | Step: 106 | LearningRate: 0.00150 \n",
      "| Reward: 68 | Episode: 503 | Qmax: 102.2576 | Exploration: 0.423193 | Step: 32 | LearningRate: 0.00150 \n",
      "| Reward: 16 | Episode: 504 | Qmax: 98.9502 | Exploration: 0.422770 | Step: 84 | LearningRate: 0.00150 \n",
      "| Reward: 47 | Episode: 505 | Qmax: 99.1445 | Exploration: 0.422347 | Step: 53 | LearningRate: 0.00150 \n",
      "| Reward: -1 | Episode: 506 | Qmax: 99.1242 | Exploration: 0.421925 | Step: 101 | LearningRate: 0.00150 \n",
      "| Reward: -24 | Episode: 507 | Qmax: 98.9204 | Exploration: 0.421503 | Step: 124 | LearningRate: 0.00150 \n",
      "| Reward: 29 | Episode: 508 | Qmax: 99.2863 | Exploration: 0.421081 | Step: 71 | LearningRate: 0.00150 \n",
      "| Reward: 39 | Episode: 509 | Qmax: 98.9104 | Exploration: 0.420660 | Step: 61 | LearningRate: 0.00150 \n",
      "| Reward: -109 | Episode: 510 | Qmax: 97.9057 | Exploration: 0.420240 | Step: 209 | LearningRate: 0.00150 \n",
      "| Reward: 46 | Episode: 511 | Qmax: 98.9098 | Exploration: 0.419819 | Step: 54 | LearningRate: 0.00150 \n",
      "| Reward: 59 | Episode: 512 | Qmax: 102.0157 | Exploration: 0.419400 | Step: 41 | LearningRate: 0.00150 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2704a5757892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mQnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdra_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2704a5757892>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, qnet, dra_policy)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0msummary_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mep_ave_max_q\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0msummary_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEXPLORATION_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     \u001b[0msummary_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 })\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \"\"\"\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5211\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5212\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5213\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from graphviz import Source\n",
    "from qnetwork import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from env_dynamic_ap import *\n",
    "# from Plot_Path import *\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from dra_planning import gen_dra_policy\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    DEVICE = \"/device:CPU:0\"\n",
    "else:\n",
    "    DEVICE = \"/device:GPU:0\"\n",
    "\n",
    "LTL = \"<>(A && <>(B && <>T))\"\n",
    "\n",
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "# GAMMA = 0.7\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 210\n",
    "MAX_EPISODES = 30000\n",
    "MAX_EPISODE_LEN = 2000\n",
    "file_appendix = \"GuideLearning_\" + time.ctime()[4:16].replace(\"  \",\"\").replace(\" \",\"_\").replace(\":\",\"-\") + LTL\n",
    "SUMMARY_DIR = './results/' + file_appendix\n",
    "SAVE_DIR = \"./saved_model/\" + file_appendix + \"/guide_learning.ckpt\"\n",
    "EXPLORATION_RATE = 0.7\n",
    "LR_DECAY_TRUNCATION = -200\n",
    "\n",
    "env = CurrentWorld(LTL)\n",
    "\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "def train(sess, env, qnet, dra_policy):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = list(np.unravel_index(s, env.shape))\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        \n",
    "        reward_list = []\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                if tuple(s) in dra_policy.keys():\n",
    "                    s2, r, terminal, info = env.step(dra_policy[tuple(s)])\n",
    "                else:\n",
    "                    s2, r, terminal, info = env.step(np.random.randint(env.nA))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.02 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.999\n",
    "                    \n",
    "                reward_list += [ep_reward]\n",
    "                \n",
    "                if np.average(reward_list[-10:]) > LR_DECAY_TRUNCATION:\n",
    "                    qnet.decay_learning_rate(1)\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE,\n",
    "                    summary_vars[3]: qnet.get_learning_rate().eval()\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} | Step: {:d} | LearningRate: {:.5f} '.format(int(ep_reward), \\\n",
    "                                                                                                    num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE, j, qnet.get_learning_rate().eval()))\n",
    "                \n",
    "                f = open(\"./stats/stats\" + file_appendix + \".txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE)\n",
    "                        + \" | Step: \" + str(j)\n",
    "                        + \" | LearningRate: \" + str(qnet.get_learning_rate().eval())\n",
    "                        + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                f = open(SUMMARY_DIR + \"/reward.txt\", \"ab\")\n",
    "                f.write(str(int(ep_reward)))\n",
    "                f.close()\n",
    "\n",
    "                break\n",
    "                \n",
    "#         if num_epi%10 == 0:\n",
    "#             state_list = []\n",
    "#             action_list = []\n",
    "#             world = np.zeros(env.shape)\n",
    "#             for state in range(env.nS):\n",
    "#                 state = np.unravel_index(state, env.shape)\n",
    "#                 action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "#                 action = np.argmax(action)\n",
    "#                 state_list.append(state)\n",
    "#                 action_list.append(action)\n",
    "                \n",
    "# #             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "#             f = open(\"action.txt\",\"ab\")\n",
    "#             act_string = np.array_str(np.reshape(action_list, env.shape))\n",
    "#             f.write(act_string)\n",
    "#             f.write(\"---------------------------\\n\")\n",
    "#             f.close()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 3\n",
    "    action_dim = env.nA\n",
    "\n",
    "    dra_policy = gen_dra_policy(LTL, env)\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, SAVE_DIR, DEVICE)\n",
    "    \n",
    "    train(sess, env, Qnet, dra_policy)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
